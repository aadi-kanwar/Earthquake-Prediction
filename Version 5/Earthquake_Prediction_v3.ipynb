{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZK-sKXsN6_Y",
        "outputId": "7b408fa5-844a-4ffc-d91d-3b78ffc3a50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3445751 entries, 0 to 3445750\n",
            "Data columns (total 12 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   time          int64  \n",
            " 1   place         object \n",
            " 2   status        object \n",
            " 3   tsunami       int64  \n",
            " 4   significance  int64  \n",
            " 5   data_type     object \n",
            " 6   magnitudo     float64\n",
            " 7   state         object \n",
            " 8   longitude     float64\n",
            " 9   latitude      float64\n",
            " 10  depth         float64\n",
            " 11  date          object \n",
            "dtypes: float64(4), int64(3), object(5)\n",
            "memory usage: 315.5+ MB\n",
            "None\n",
            "           time                                 place    status  tsunami  \\\n",
            "0  631153353990     12 km NNW of Meadow Lakes, Alaska  reviewed        0   \n",
            "1  631153491210            14 km S of Volcano, Hawaii  reviewed        0   \n",
            "2  631154083450            7 km W of Cobb, California  reviewed        0   \n",
            "3  631155512130  11 km E of Mammoth Lakes, California  reviewed        0   \n",
            "4  631155824490                16km N of Fillmore, CA  reviewed        0   \n",
            "\n",
            "   significance   data_type  magnitudo        state   longitude   latitude  \\\n",
            "0            96  earthquake       2.50       Alaska -149.669200  61.730200   \n",
            "1            31  earthquake       1.41       Hawaii -155.212333  19.317667   \n",
            "2            19  earthquake       1.11   California -122.806167  38.821000   \n",
            "3            15  earthquake       0.98   California -118.846333  37.664333   \n",
            "4           134  earthquake       2.95   California -118.934000  34.546000   \n",
            "\n",
            "    depth                              date  \n",
            "0  30.100  1990-01-01 00:22:33.990000+00:00  \n",
            "1   6.585  1990-01-01 00:24:51.210000+00:00  \n",
            "2   3.220  1990-01-01 00:34:43.450000+00:00  \n",
            "3  -0.584  1990-01-01 00:58:32.130000+00:00  \n",
            "4  16.122  1990-01-01 01:03:44.490000+00:00  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# file_path = \"Earthquake_Data.csv\"\n",
        "file_path = \"Eartquakes-1990-2023.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information and the first few rows\n",
        "print(df.info())\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[df['data_type'] == 'earthquake']\n",
        "# df = df.drop(['place','status','state','time','date', 'tsunami',  ], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           time                                 place    status  tsunami  \\\n",
            "0  631153353990     12 km NNW of Meadow Lakes, Alaska  reviewed        0   \n",
            "1  631153491210            14 km S of Volcano, Hawaii  reviewed        0   \n",
            "2  631154083450            7 km W of Cobb, California  reviewed        0   \n",
            "3  631155512130  11 km E of Mammoth Lakes, California  reviewed        0   \n",
            "4  631155824490                16km N of Fillmore, CA  reviewed        0   \n",
            "\n",
            "   significance   data_type  magnitudo        state   longitude   latitude  \\\n",
            "0            96  earthquake       2.50       Alaska -149.669200  61.730200   \n",
            "1            31  earthquake       1.41       Hawaii -155.212333  19.317667   \n",
            "2            19  earthquake       1.11   California -122.806167  38.821000   \n",
            "3            15  earthquake       0.98   California -118.846333  37.664333   \n",
            "4           134  earthquake       2.95   California -118.934000  34.546000   \n",
            "\n",
            "    depth                              date  \n",
            "0  30.100  1990-01-01 00:22:33.990000+00:00  \n",
            "1   6.585  1990-01-01 00:24:51.210000+00:00  \n",
            "2   3.220  1990-01-01 00:34:43.450000+00:00  \n",
            "3  -0.584  1990-01-01 00:58:32.130000+00:00  \n",
            "4  16.122  1990-01-01 01:03:44.490000+00:00  \n"
          ]
        }
      ],
      "source": [
        "# df = df.drop(['data_type'], axis=1)\n",
        "# print(df['data_type'].unique())\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_\n",
        "# # Calculate the correlation matrix\n",
        "# correlation_matrix = df.corr()\n",
        "\n",
        "# # Extract the correlation of the 'Mag' column with other columns\n",
        "# correlation_with_mag = correlation_matrix[\"Mag\"]\n",
        "\n",
        "# # Display the correlation values\n",
        "# correlation_with_mag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xHHMCJHgOKlj",
        "outputId": "76e5d08d-8a2f-4a9b-df92-7382c34b09ae"
      },
      "outputs": [],
      "source": [
        "# # Attempt to split the single column into multiple columns based on spaces\n",
        "# df_cleaned = df.iloc[:, 0].str.split(r'\\s+', expand=True)\n",
        "\n",
        "# # Display first few rows to check the structure\n",
        "# df_cleaned.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRnOTYWSON1Q",
        "outputId": "2beb49bd-eec6-4753-f67b-aad39efeefb7"
      },
      "outputs": [],
      "source": [
        "# # Assign proper column names\n",
        "# df_cleaned.columns = [\n",
        "#     \"Date\", \"Time\", \"Latitude\", \"Longitude\", \"Depth\", \"Mag\", \"Magt\",\n",
        "#     \"Nst\", \"Gap\", \"Clo\", \"RMS\", \"SRC\", \"EventID\"\n",
        "# ]\n",
        "\n",
        "# # Select relevant features and target variable\n",
        "# df_model = df_cleaned[[\"Latitude\", \"Longitude\", \"Nst\", \"Mag\"]]\n",
        "\n",
        "# # Convert data types to numeric\n",
        "# df_model = df_model.astype({\"Latitude\": float, \"Longitude\": float, \"Nst\": int, \"Mag\": float})\n",
        "\n",
        "# # Display dataset info and first few rows\n",
        "# df_model.info(), df_model.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHyjLIdeOPyE",
        "outputId": "6bcbbcee-4de5-4816-b52b-d81d739f7755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7196255044132028, 0.9565859077527649)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Define features and target variable\n",
        "X = df[[\"latitude\", \"longitude\", \"depth\"]]\n",
        "y = df[\"magnitudo\"]\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "mae, rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sklearn\n",
        "# from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "\n",
        "# # Ensure df is defined before using it\n",
        "# if 'df' not in locals():\n",
        "#     raise ValueError(\"The dataframe 'df' is not defined.\")\n",
        "\n",
        "# # Define features and target variable\n",
        "# X = df[[\"latitude\", \"longitude\", \"depth\"]]\n",
        "# y = df[\"magnitudo\"]\n",
        "\n",
        "# # Split data into training and testing sets (80% train, 20% test)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Scale the features\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # Define the model\n",
        "# rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# # Define the hyperparameters and their values\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 500],  # Increasing to 500 trees for better performance\n",
        "#     'max_depth': [None, 10, 20, 30],  # Different depths to prevent overfitting or underfitting\n",
        "#     'min_samples_split': [2, 5, 10],  # Higher values help avoid overfitting\n",
        "#     'min_samples_leaf': [1, 2, 5],  # Prevents overfitting by forcing larger leaves\n",
        "#     'max_features': ['sqrt', 'log2'],  # Common choices for Random Forest\n",
        "#     'bootstrap': [True, False],  # You can test both bootstrap sampling options\n",
        "#     'oob_score': [True, False],  # Enable OOB scoring for internal validation\n",
        "#     'n_jobs': [-1]  # Use all cores for parallelism\n",
        "# }\n",
        "\n",
        "# # Create the GridSearchCV object\n",
        "# grid_search = GridSearchCV(\n",
        "#     estimator=rf,\n",
        "#     param_grid=param_grid,\n",
        "#     cv=3,\n",
        "#     n_jobs=4,\n",
        "#     verbose=1  # This will show progress as each set of parameters is tested\n",
        "# )\n",
        "\n",
        "# # Fit the model with grid search\n",
        "# grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# # Output the best parameters found by GridSearchCV\n",
        "# print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# # Train the model with the best parameters\n",
        "# best_rf = grid_search.best_estimator_\n",
        "\n",
        "# # Make predictions\n",
        "# y_pred = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# # Evaluate the model\n",
        "# mae = mean_absolute_error(y_test, y_pred)\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# rmse = mse ** 0.5\n",
        "\n",
        "# print(f\"MAE: {mae}, RMSE: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_iterations: 13\n",
            "n_required_iterations: 13\n",
            "n_possible_iterations: 13\n",
            "min_resources_: 656\n",
            "max_resources_: 2689476\n",
            "aggressive_elimination: False\n",
            "factor: 2\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 5120\n",
            "n_resources: 656\n",
            "Fitting 3 folds for each of 5120 candidates, totalling 15360 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
            "6720 fits failed out of a total of 15360.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "3840 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "2880 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 448, in fit\n",
            "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ...        nan        nan        nan]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ...        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 1\n",
            "n_candidates: 2560\n",
            "n_resources: 1312\n",
            "Fitting 3 folds for each of 2560 candidates, totalling 7680 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.70134019 0.69768078 0.69768078]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.89355851 0.89901877 0.89901877]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 2\n",
            "n_candidates: 1280\n",
            "n_resources: 2624\n",
            "Fitting 3 folds for each of 1280 candidates, totalling 3840 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.72306926 0.72306926 0.72306926]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.84617712 0.84617712 0.84617712]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 3\n",
            "n_candidates: 640\n",
            "n_resources: 5248\n",
            "Fitting 3 folds for each of 640 candidates, totalling 1920 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.72704855 0.72704855 0.72704855]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83076322 0.83076322 0.83076322]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 4\n",
            "n_candidates: 320\n",
            "n_resources: 10496\n",
            "Fitting 3 folds for each of 320 candidates, totalling 960 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.75369042 0.75369042 0.75369042]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.84625282 0.84625282 0.84625282]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 5\n",
            "n_candidates: 160\n",
            "n_resources: 20992\n",
            "Fitting 3 folds for each of 160 candidates, totalling 480 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.76465097 0.76465097 0.76465097]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.85359412 0.85359412 0.85359412]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 6\n",
            "n_candidates: 80\n",
            "n_resources: 41984\n",
            "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.75967576 0.7595822  0.7595822 ]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.8539466  0.83947335 0.83947335]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 7\n",
            "n_candidates: 40\n",
            "n_resources: 83968\n",
            "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.77150068 0.7712649  0.7712649 ]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83797946 0.85031532 0.85031532]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 8\n",
            "n_candidates: 20\n",
            "n_resources: 167936\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.77967246 0.77983785 0.77983785]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83830941 0.83856782 0.83856782]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 9\n",
            "n_candidates: 10\n",
            "n_resources: 335872\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.7823922  0.78253928 0.78253928]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83746243 0.83761892 0.83761892]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 10\n",
            "n_candidates: 5\n",
            "n_resources: 671744\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.78521072 0.78531207 0.78531207]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83377998 0.83391079 0.83391079]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 11\n",
            "n_candidates: 3\n",
            "n_resources: 1343488\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.79233731 0.79244234 0.79244234]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83071755 0.83078454 0.83078454]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "iter: 12\n",
            "n_candidates: 2\n",
            "n_resources: 2686976\n",
            "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.66407809 0.66407809 0.66799468 ... 0.79244234 0.79488887 0.79488887]\n",
            "  warnings.warn(\n",
            "c:\\B Tech\\MLOps Project shii\\earthquake\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the train scores are non-finite: [0.95166521 0.95166521 0.9524981  ... 0.83078454 0.82635418 0.82635418]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'bootstrap': True, 'max_depth': 20, 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 500, 'n_jobs': 4, 'oob_score': True, 'random_state': 42}\n",
            "MAE: 0.403033661331168, RMSE: 0.5901735538433256\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.experimental import enable_halving_search_cv  # Enable the HalvingGridSearchCV\n",
        "from sklearn.model_selection import HalvingGridSearchCV  # Now you can import it\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Ensure df is defined before using it\n",
        "if 'df' not in locals():\n",
        "    raise ValueError(\"The dataframe 'df' is not defined.\")\n",
        "\n",
        "# Define features and target variable\n",
        "X = df[[\"latitude\", \"longitude\", \"depth\"]]\n",
        "y = df[\"magnitudo\"]\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameters and their values\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500, 1000],  # More trees for better model performance\n",
        "    'max_depth': [None, 10, 20, 30, 40],  # Explore different tree depths\n",
        "    'min_samples_split': [2, 5, 10, 20],  # Minimum number of samples for a split\n",
        "    'min_samples_leaf': [1, 2, 5, 10],  # Minimum number of samples for a leaf node\n",
        "    'max_features': ['sqrt', 'log2', None, 'auto'],  # Explore feature options\n",
        "    'bootstrap': [True, False],  # Test with and without bootstrap sampling\n",
        "    'oob_score': [True, False],  # Test with and without Out-Of-Bag scoring\n",
        "    'n_jobs': [4],  # Use 4 cores for parallelism\n",
        "    'random_state': [42]  # Ensure consistency across runs\n",
        "}\n",
        "\n",
        "# Create the HalvingGridSearchCV object\n",
        "halving_grid_search = HalvingGridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    n_jobs=4,  # Set n_jobs to 4 for parallelism\n",
        "    verbose=1,  # Show progress for each set of parameters\n",
        "    factor=2,  # Halve the number of candidates at each iteration\n",
        "    max_resources='auto'  # Automatically adjust resources to avoid overloading\n",
        ")\n",
        "\n",
        "# Fit the model with halving grid search\n",
        "halving_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Output the best parameters found by HalvingGridSearchCV\n",
        "print(\"Best parameters found: \", halving_grid_search.best_params_)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_rf = halving_grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "print(f\"MAE: {mae}, RMSE: {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'bootstrap': True, 'max_depth': 20, 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 500, 'n_jobs': 4, 'oob_score': True, 'random_state': 42}\n"
          ]
        }
      ],
      "source": [
        "best_params = halving_grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Define features and target variable\n",
        "X = df[[\"latitude\", \"longitude\", \"depth\"]]\n",
        "y = df[\"magnitudo\"]\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Define the hyperparameters and their values\n",
        "param_grid = {\n",
        "    'fit_intercept': [True, False],\n",
        "    'normalize': [True, False]\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_lr = LinearRegression(**best_params)\n",
        "best_lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_lr.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "mae, rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIwa_A6COXTE",
        "outputId": "10b7b8ad-5021-4f81-c21e-46c5567751db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid input! Please enter numerical values for Latitude, Longitude, and Nst.\n"
          ]
        }
      ],
      "source": [
        "# # Function to make predictions on user input\n",
        "# def predict_earthquake(latitude, longitude, nst):\n",
        "#     input_data = scaler.transform([[latitude, longitude, nst]])\n",
        "#     predicted_mag = model.predict(input_data)[0]\n",
        "#     return round(predicted_mag, 2)\n",
        "\n",
        "# # Function to take user input and predict earthquake magnitude\n",
        "# def user_input_prediction():\n",
        "#     try:\n",
        "#         latitude = float(input(\"Enter Latitude: \"))\n",
        "#         longitude = float(input(\"Enter Longitude: \"))\n",
        "#         nst = int(input(\"Enter Number of Stations (Nst): \"))\n",
        "        \n",
        "#         predicted_magnitude = predict_earthquake(latitude, longitude, nst)\n",
        "#         print(f\"Predicted Earthquake Magnitude: {predicted_magnitude}\")\n",
        "    \n",
        "#     except ValueError:\n",
        "#         print(\"Invalid input! Please enter numerical values for Latitude, Longitude, and Nst.\")\n",
        "\n",
        "# # Call the function to allow user input\n",
        "# user_input_prediction()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInvalid input! Please enter numerical values for Latitude, Longitude, and Depth.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Call the function to allow user input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43muser_input_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36muser_input_prediction\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34muser_input_prediction\u001b[39m():\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         latitude = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEnter Latitude: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m         longitude = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnter Longitude: \u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     15\u001b[39m         depth = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnter Depth: \u001b[39m\u001b[33m\"\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to make predictions on user input\n",
        "def predict_earthquake(latitude, longitude, depth):\n",
        "    input_data = pd.DataFrame([[latitude, longitude, depth]], columns=['latitude', 'longitude', 'depth'])\n",
        "    input_data_scaled = scaler.transform(input_data)\n",
        "    predicted_mag = model.predict(input_data_scaled)[0]\n",
        "    return round(predicted_mag, 2)\n",
        "\n",
        "# Function to take user input and predict earthquake magnitude\n",
        "def user_input_prediction():\n",
        "    try:\n",
        "        latitude = float(input(\"Enter Latitude: \"))\n",
        "        longitude = float(input(\"Enter Longitude: \"))\n",
        "        depth = float(input(\"Enter Depth: \"))\n",
        "        \n",
        "        predicted_magnitude = predict_earthquake(latitude, longitude, depth)\n",
        "        print(f\"Predicted Earthquake Magnitude: {predicted_magnitude}\")\n",
        "    \n",
        "    except ValueError:\n",
        "        print(\"Invalid input! Please enter numerical values for Latitude, Longitude, and Depth.\")\n",
        "\n",
        "# Call the function to allow user input\n",
        "user_input_prediction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Earthquake Magnitude: 1.55\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Load the trained Random Forest model and scaler\n",
        "model = joblib.load(\"earthquake_model2_rf.pkl\")\n",
        "scaler = joblib.load(\"scaler2.pkl\")\n",
        "\n",
        "# Function to make predictions on user input\n",
        "def predict_earthquake_rf(latitude, longitude, depth):\n",
        "    input_data = pd.DataFrame([[latitude, longitude, depth]], columns=['latitude', 'longitude', 'depth'])\n",
        "    input_data_scaled = scaler.transform(input_data)\n",
        "    predicted_mag = model.predict(input_data_scaled)[0]\n",
        "    return round(predicted_mag, 2)\n",
        "\n",
        "# Function to take user input and predict earthquake magnitude using Random Forest\n",
        "def user_input_prediction_rf():\n",
        "    try:\n",
        "        latitude = float(input(\"Enter Latitude: \"))\n",
        "        longitude = float(input(\"Enter Longitude: \"))\n",
        "        depth = float(input(\"Enter Depth: \"))\n",
        "        \n",
        "        predicted_magnitude = predict_earthquake_rf(latitude, longitude, depth)\n",
        "        print(f\"Predicted Earthquake Magnitude: {predicted_magnitude}\")\n",
        "    \n",
        "    except ValueError:\n",
        "        print(\"Invalid input! Please enter numerical values for Latitude, Longitude, and Depth.\")\n",
        "\n",
        "# Call the function to allow user input\n",
        "user_input_prediction_rf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pY258mw5OjtS"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the trained model\n",
        "with open(\"earthquake_model2_rf.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump(best_rf, model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7myiXfLRRz9_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaler2.pkl']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(best_rf, \"earthquake_model2_rf.pkl\")\n",
        "\n",
        "# Save the StandardScaler\n",
        "joblib.dump(scaler, \"scaler2.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "earthquake",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
